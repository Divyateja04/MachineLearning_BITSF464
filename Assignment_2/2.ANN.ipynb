{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B\n",
    "In general, a basic neural network architecture can be considered that consists of an input  \n",
    "layer, one or more hidden layers, and an output layer.  \n",
    "\n",
    "You  are  supposed  to  build  15  distinct  artificial  neural  network  classifiers  by  varying  one  or \n",
    "more paramours from the following list: \n",
    "\n",
    "- (i). Number of hidden layers â€“ 2 or 3\n",
    "- (ii) Total number of neurons in the hidden layer is 100 or 150\n",
    "- (iii) Activation function is from any of the following functions: tanh, sigmoid, ReLu\n",
    "\n",
    "---\n",
    "\n",
    "You need to train your network on the MNIST dataset. You can use any optimization algorithm \n",
    "like  stochastic  gradient  descent  or  Adam  optimizer.  You  need  to  evaluate  your  network's \n",
    "performance on a test set of images from the MNIST dataset. You can calculate the accuracy and \n",
    "confusion matrix to measure your network's performance. \n",
    "Perform a comparative study of these 15 models and figure out the best classifier. Do you have \n",
    "a classifier that  is not statistically significant from the best classifier? Detail the results with all \n",
    "explanations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Importing TorchVision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one of the data set\n",
    "image, label = train_data[0]\n",
    "\n",
    "# Print the image\n",
    "plt.matshow(image[0])\n",
    "\n",
    "# Print the output\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the size of datasets\n",
    "len(train_data.data), len(test_data.data), len(train_data.targets), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What this step is practically doing is converting all the data into batches of 32\n",
    "# And returning the iterables to us\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_data_loader}, {test_data_loader}\")\n",
    "print(f\"Length of train dataloader: {len(train_data_loader)} batches of {100}\")\n",
    "print(f\"Length of test dataloader: {len(test_data_loader)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_layers, hidden_units, activation_func, output_shape):\n",
    "        # Call the super class's init function\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            self.layer_stack.append(nn.Linear(in_features=self.hidden_units, out_features=self.hidden_units))\n",
    "            # self.hidden_units -= 20\n",
    "            if activation_func == \"t\":\n",
    "                self.layer_stack.append(nn.Tanh())\n",
    "            elif activation_func == \"s\":\n",
    "                self.layer_stack.append(nn.Sigmoid())\n",
    "            elif activation_func == \"r\":\n",
    "                self.layer_stack.append(nn.ReLU())\n",
    "        \n",
    "        self.layer_stack.append(nn.Linear(in_features=self.hidden_units, out_features=output_shape))\n",
    "        self.layer_stack.append(nn.LogSoftmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistModel = MNISTModel(\n",
    "    input_shape=784,\n",
    "    hidden_layers=3,\n",
    "    hidden_units=100,\n",
    "    output_shape=10,\n",
    "    activation_func=\"s\"\n",
    ")\n",
    "mnistModel.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------Training---------\n",
    "epochs = 10\n",
    "\n",
    "# Create the training and testing loop\n",
    "for epoch in range(epochs):    \n",
    "        \n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        # Train the model\n",
    "        mnistModel.train()\n",
    "        # Generate Value\n",
    "        y_pred = mnistModel(X)\n",
    "        # Generate loss from loss function\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Optimize :)\n",
    "        # Apparently this sets all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # if (batch+1) % 2 == 0:\n",
    "        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # We loop through all the test data\n",
    "    for images, labels in test_data_loader:\n",
    "        # Generate output for one model\n",
    "        outputs = mnistModel(images)\n",
    "        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "metrics.confusion_matrix(y_true, y_pred)\n",
    "print(\"Classification report for ANN :\\n%s\\n\"\n",
    "      % (metrics.classification_report(y_true, y_pred)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [2, 3]\n",
    "neurons_in_hidden_layers = [100, 150]\n",
    "activation_functions = [\"t\", \"s\", \"r\"]\n",
    "epochs = 100\n",
    "model_no = 0\n",
    "\n",
    "for hs in hidden_sizes:\n",
    "    for nihl in neurons_in_hidden_layers:\n",
    "        for af in activation_functions:\n",
    "            model_no += 1\n",
    "            \n",
    "            mnistModel = MNISTModel(\n",
    "                input_shape=784,\n",
    "                hidden_layers=hs,\n",
    "                hidden_units=nihl,\n",
    "                output_shape=10,\n",
    "                activation_func=af\n",
    "            )\n",
    "            \n",
    "            print(f\"Model #{model_no}\")\n",
    "            \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)\n",
    "\n",
    "            # ---------Training---------\n",
    "            # Create the training and testing loop\n",
    "            for epoch in range(epochs):    \n",
    "                    \n",
    "                for batch, (X, y) in enumerate(train_data_loader):\n",
    "                    # Train the model\n",
    "                    mnistModel.train()\n",
    "                    # Generate Value\n",
    "                    y_pred = mnistModel(X)\n",
    "                    # Generate loss from loss function\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    # Optimize :)\n",
    "                    # Apparently this sets all gradients to zero\n",
    "                    optimizer.zero_grad()\n",
    "                    # Back Propagate\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Debugging Purposes only, Do not use in the loop\n",
    "                    # if (batch+1) % 100 == 0:\n",
    "                    #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():                \n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "\n",
    "                # We loop through all the test data\n",
    "                for images, labels in test_data_loader:\n",
    "                    # Generate output for one model\n",
    "                    outputs = mnistModel(images)\n",
    "                    # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    y_pred.append(predicted[0])\n",
    "                    y_true.append(labels[0])\n",
    "\n",
    "            metrics.confusion_matrix(y_true, y_pred)\n",
    "            print(f\"Classification report for Hidden Layers: {hs}, Neurons in Each Hidden Layer: {nihl}, Activation Function: {af} :\\n%s\\n\"\n",
    "                % (metrics.classification_report(y_true, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
